# -*- coding: utf-8 -*-
"""data_pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MzN4MpBRM02Ojby5uvanTwYzMDiIU5aG
"""

import pandas as pd
import numpy as np
from typing import Tuple


def load_spy_data(csv_path: str) -> pd.DataFrame:
    """
    Load SPY data from a CSV file and return a DataFrame
    indexed by Date, sorted in ascending order.

    """
    df = pd.read_csv(csv_path)

    if 'Date' not in df.columns:
        raise ValueError("CSV must contain a 'Date' column.")

    df['Date'] = pd.to_datetime(df['Date'])
    df = df.set_index('Date').sort_index()

    return df


def add_returns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure the DataFrame has a 'ret' column with daily log returns.

    """
    # Case 1: already have ret
    if 'ret' in df.columns:
        return df

    # Case 2: file already has returns column
    returns_col = None
    for col in df.columns:
        if 'return' in col.lower():
            returns_col = col
            break

    if returns_col is not None:
        df = df.rename(columns={returns_col: 'ret'})
        return df

    # Case 3: compute returns from a price column
    price_col = None
    for col in df.columns:
        if col.lower() in ['adj close', 'adj_close', 'close']:
            price_col = col
            break

    if price_col is None:
        raise ValueError("No 'ret' or returns column and no price column like 'Adj Close' or 'Close' present.")

    df['ret'] = np.log(df[price_col] / df[price_col].shift(1))
    df = df.dropna(subset=['ret'])

    return df


def add_realized_vol(df: pd.DataFrame, window: int = 21) -> pd.DataFrame:
    """
    Add a realized volatility column 'realized_vol' using a rolling
    standard deviation of returns over a given window length (in days).
    """
    if 'ret' not in df.columns:
        raise ValueError("DataFrame must have a 'ret' column before computing realized volatility.")

    df['realized_vol'] = df['ret'].rolling(window).std()
    df = df.dropna(subset=['realized_vol'])
    return df


def train_test_split(
    df: pd.DataFrame,
    split_date: str
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Split the DataFrame into train and test sets based on a split date.

    """
    train_df = df.loc[df.index < split_date].copy()
    test_df = df.loc[df.index >= split_date].copy()

    return train_df, test_df


def make_windowed_dataset(
    returns: np.ndarray,
    vols: np.ndarray,
    window_size: int
) -> Tuple[np.ndarray, np.ndarray]:

    X_list = []
    y_list = []

    for i in range(window_size, len(returns)):
        window = returns[i - window_size:i]
        X_list.append(window.reshape(-1, 1))
        y_list.append(vols[i])

    X = np.array(X_list)
    y = np.array(y_list)
    return X, y


def build_spy_pipeline(
    csv_path: str,
    split_date: str = "2020-01-01",
    vol_window: int = 21,
    seq_window: int = 20
):

    df = load_spy_data(csv_path)
    df = add_returns(df)
    df = add_realized_vol(df, window=vol_window)

    train_df, test_df = train_test_split(df, split_date)

    train_returns = train_df['ret'].values
    train_vols = train_df['realized_vol'].values
    test_returns = test_df['ret'].values
    test_vols = test_df['realized_vol'].values

    X_train_seq, y_train_seq = make_windowed_dataset(train_returns, train_vols, seq_window)
    X_test_seq, y_test_seq = make_windowed_dataset(test_returns, test_vols, seq_window)

    return train_df, test_df, X_train_seq, y_train_seq, X_test_seq, y_test_seq